{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AC_mode3.ipynb","provenance":[],"collapsed_sections":["STR4M5oLMRML","NWu7gtY4PPv4","GI_kANkKEnmQ","_lIT_ZxfE1mZ","xaIOGJ22LzA6","cLWWWfE6L20n","fK7NGsGKL5Ts","Zm7qDbXsMJcD","i4q7BrEyjTh2","ZuSOHFh7PwIK"],"authorship_tag":"ABX9TyMDQdQAeJ4DZZFssE7XMgbC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ogs2EgjWv8xj"},"source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","path = \"/content/drive/My Drive/GAN_for_Neural_Graph\"\n","\n","os.chdir(path)\n","os.listdir(path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"STR4M5oLMRML"},"source":["#Import Libraries"]},{"cell_type":"code","metadata":{"id":"vaXLNPtREXXL"},"source":["import pandas as pd\n","import os\n","import numpy as np\n","import keras\n","from numpy import zeros\n","from numpy import ones\n","from numpy import expand_dims\n","from numpy.random import randn\n","from numpy.random import randint\n","from keras import optimizers\n","from keras import backend as K\n","from keras import activations\n","from keras import initializers\n","from keras import regularizers\n","from keras import constraints\n","from keras import Sequential\n","from keras import backend\n","from keras.layers import Input\n","from keras.layers import Dense\n","from keras.layers import Reshape\n","from keras.layers import Flatten\n","from keras.layers import Conv2D\n","from keras.layers import Conv2DTranspose\n","from keras.layers import LeakyReLU\n","from keras.layers import BatchNormalization\n","from keras.layers import Dropout\n","from keras.layers import Embedding\n","from keras.layers import Activation\n","from keras.layers import Concatenate\n","from keras.layers import Add\n","from keras.utils import conv_utils\n","from keras.utils import to_categorical\n","from keras.engine import Layer\n","from keras.engine import InputSpec\n","from keras.datasets.fashion_mnist import load_data\n","from keras.constraints import Constraint\n","from keras.initializers import RandomNormal\n","from keras.optimizers import Adam, RMSprop\n","from keras.models import Model\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MultiLabelBinarizer\n","from matplotlib import pyplot"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NWu7gtY4PPv4"},"source":["# Load data"]},{"cell_type":"code","metadata":{"id":"ciiXDHH5DWfV"},"source":["table = pd.read_csv(\"./Dataset/left_table.csv\")\n","table1 = table.loc[:,['Id','label_id']]\n","print(table1.head())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nDm6yzX-DWfX"},"source":["table_dict = dict(zip(table1.Id, table1.label_id))\n","# print(table_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DVVM9aBzDWfX"},"source":["csv_files = os.listdir(\"./Dataset/FC_norm\")\n","csv_files = [file for file in csv_files if file[-1]=='v']\n","print(len(csv_files))\n","\n","X = []\n","y = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7TnyI1ScDWfY"},"source":["for file in csv_files:\n","    df = pd.read_csv(os.path.join(\"./Dataset/FC_norm\",file),header = None)\n","    X.append(df.to_numpy())\n","    key = int(file.strip('.csv'))\n","#     print(key)\n","    y.append(int(table_dict[key]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fb4ANP-1PWBW"},"source":["X = np.array(X)\n","print(X.shape)\n","y = np.array(y)\n","print(y.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pqD_gin1PYRC"},"source":["def loadDataset():\n","    return X,y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AOy7GruRPZQC"},"source":["table2 = table.loc[:,['label_id', 'Age', 'Gender']]\n","print(table2.head())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YBStP1tpPapw"},"source":["#Normalize Age column\n","table2['Age'] = (table2['Age'] - table2['Age'].min()) / (table2['Age'].max() - table2['Age'].min())\n","\n","#Make Gender 0, 1 instead of 1, 0\n","table2['Gender'] = table2['Gender'].replace([2],0)\n","print(table2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZJHdGKEWDWfZ"},"source":["#One-hot encose the class labels\n","class_labels = np.array(list(table2['label_id']))\n","class_labels = np.expand_dims(class_labels, axis=1)\n","\n","mlb = MultiLabelBinarizer()\n","class_labels = mlb.fit_transform(class_labels)\n","print(class_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zRwYLlEHDWfa"},"source":["encoded_data = []\n","for label, row in zip(class_labels, table2.itertuples()):\n","    label = list(label)\n","    age = [row.Age]\n","    gender = [row.Gender]\n","#     print(type(label), type(age), type(gender))\n","#     print(label, age, gender)\n","    encoded_data.append(label+age+gender)\n","    #print(label+age+gender)\n","\n","# encoded_data = np.array(encoded_data)\n","for i in range(2):\n","    print(encoded_data[i])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"USQE-oecDWfa"},"source":["enc_idx = np.arange(0,len(encoded_data))\n","\n","sample_idx = np.random.choice(enc_idx, size = 10)\n","\n","print(sample_idx)\n","\n","for idx in sample_idx:\n","    print(encoded_data[idx])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FUqXXwzMDWfa"},"source":["sample_idx = np.random.choice(len(encoded_data), size = 10)\n","print(sample_idx)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GI_kANkKEnmQ"},"source":["#Hyper parameters"]},{"cell_type":"code","metadata":{"id":"c7d8ctCfwNb9"},"source":["# Discriminator\n","DECAY = 0.0005\n","ALPHA = 0.33\n","LR_D = 0.0001\n","BETA_D = 0.5\n","FE_CHANNEL = 128\n","MERGE_CHANNEL = 64\n","\n","\n","# Generator\n","D = 10\n","STD = 0.02\n","\n","# GAN\n","LR_G = 0.0001\n","BETA_G = 0.5\n","\n","# Loss weights\n","LOSS_WEIGHTS = [1.0, 1.0]\n","\n","# Train function\n","LATENT_DIM = 50\n","BATCH_SIZE = 64\n","\n","'''\n","The number of E2E layers\n","Channel size of E2E layers\n","Dropout\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_lIT_ZxfE1mZ"},"source":["#Model function"]},{"cell_type":"code","metadata":{"id":"kYW-Fke9E5tT"},"source":["# define E2E layer\n","from keras import backend as K\n","from keras import activations\n","from keras import initializers\n","from keras import regularizers\n","from keras import constraints\n","from keras.engine import Layer\n","from keras.engine import InputSpec\n","from keras.utils import conv_utils\n","\n","class E2E_conv(Layer):\n","  def __init__(self, rank,\n","         filters,\n","         kernel_size,\n","         strides=1,\n","         padding='valid',\n","         activation=None,\n","         kernel_initializer='glorot_uniform',\n","         kernel_regularizer=None,\n","         kernel_constraint=None,\n","         **kwargs):\n","    super(E2E_conv, self).__init__(**kwargs)\n","    self.rank = rank\n","    self.filters = filters\n","    self.kernel_size = conv_utils.normalize_tuple(kernel_size, rank, 'kernel_size')\n","    self.strides = conv_utils.normalize_tuple(strides, rank, 'strides')\n","    self.padding = conv_utils.normalize_padding(padding)\n","    self.activation = activations.get(activation)\n","    self.kernel_initializer = initializers.get(kernel_initializer)\n","    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n","    self.kernel_constraint = constraints.get(kernel_constraint)\n","    self.input_spec = InputSpec(ndim=self.rank + 2)\n","\n","  def build(self, input_shape):\n","    channel_axis = -1\n","    if input_shape[channel_axis] is None:\n","      raise ValueError('The channel dimension of the inputs'\n","               'should be defined. Found `None`.')\n","    input_dim = input_shape[channel_axis]\n","    kernel_shape = self.kernel_size + (input_dim, self.filters)\n","\n","    self.kernel = self.add_weight(shape=kernel_shape,\n","                    initializer=self.kernel_initializer,\n","                    name='kernel',\n","                    regularizer=self.kernel_regularizer,\n","                    constraint=self.kernel_constraint)\n","    \n","    # Set input spec.\n","    self.input_spec = InputSpec(ndim=self.rank + 2,\n","                   axes={channel_axis:input_dim})\n","    self.built = True\n","\n","  def call(self, inputs):\n","    kernel_shape = K.get_value(self.kernel).shape\n","    d = kernel_shape[1]\n","    kernellxd = K.reshape(self.kernel[0,:], (1, kernel_shape[1], kernel_shape[2], kernel_shape[3]))  # row vector\n","    kerneldxl = K.reshape(self.kernel[1,:], (kernel_shape[1], 1, kernel_shape[2], kernel_shape[3]))  # column vector\n","    convlxd = K.conv2d(\n","        inputs,\n","        kernellxd,\n","        strides=self.strides,\n","        padding=self.padding)\n","    convdxl = K.conv2d(\n","        inputs,\n","        kerneldxl,\n","        strides=self.strides,\n","        padding=self.padding)\n","    concat1 = K.concatenate([convdxl]*d, axis=1)\n","    concat2 = K.concatenate([convlxd]*d, axis=2)\n","    return concat1 + concat2\n","\n","  def compute_output_shape(self, input_shape):\n","    return (input_shape[0], input_shape[1], input_shape[2], self.filters)\n","\n","  def get_config(self):\n","    config = {\n","        'rank': self.rank,\n","        'filters': self.filters,\n","        'kernel_size': self.kernel_size,\n","        'strides': self.strides,\n","        'padding': self.padding,\n","        'activation': activations.serialize(self.activation),\n","        'kernel_initializer': initializers.serialize(self.kernel_initializer),\n","        'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n","        'kernel_constraint': constraints.serialize(self.kernel_constraint)\n","    }\n","    base_config = super(E2E_conv, self).get_config()\n","    return dict(list(base_config.items()) + list(config.items()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xaIOGJ22LzA6"},"source":["# define D"]},{"cell_type":"code","metadata":{"id":"ySQCRi_RFACM"},"source":["# define the standalone discriminator model\n","def define_discriminator(image_shape=(200,200,1), n_classes=3):\n","  # weight regularization\n","  reg = regularizers.l2(DECAY)\n","  # weight initialization\n","  kernel_init = initializers.he_uniform()\n","  # image input\n","  in_image = Input(shape=image_shape, name='in_image')\n","\n","  # E2E layer\n","  fe = E2E_conv(2, 32, (2, 200), kernel_regularizer=reg)(in_image)  \n","  fe = BatchNormalization()(fe)\n","  fe = LeakyReLU(alpha=ALPHA)(fe)\n","\n","  fe = E2E_conv(2, 64, (2, 200), kernel_regularizer=reg)(fe)     \n","  fe = BatchNormalization()(fe)\n","  fe = LeakyReLU(alpha=ALPHA)(fe)\n","  fe = Dropout(0.5)(fe)\n","\n","  # E2N layer\n","  temp1 = Conv2D(128, (1, 200), kernel_regularizer=reg, name='row')(fe)  \n","  temp2 = Conv2D(128, (200, 1), kernel_regularizer=reg, name='column')(fe)\n","  temp2 = Reshape((200, 1, 128))(temp2)\n","  fe = Add()([temp1, temp2])\n","  fe = BatchNormalization()(fe)                          \n","  fe = LeakyReLU(alpha=ALPHA)(fe)\n","  fe = Dropout(0.5)(fe)\n","\n","  # N2G layer\n","  fe = Conv2D(256, (200, 1), kernel_regularizer=reg)(fe) \n","  fe = BatchNormalization()(fe)          \n","  fe = LeakyReLU(alpha=ALPHA)(fe)\n","  fe = Dropout(0.5)(fe)\n","\n","  # flatten feature maps\n","  fe = Flatten()(fe)\n","\n","  fe = Dense(FE_CHANNEL, kernel_regularizer=reg, kernel_initializer=kernel_init)(fe)\n","  fe = LeakyReLU(alpha=ALPHA)(fe)\n","  fe = Dropout(0.5)(fe)\n","\n","  merge = Dense(MERGE_CHANNEL, kernel_regularizer=reg, kernel_initializer=kernel_init)(fe)\n","  merge = LeakyReLU(alpha=ALPHA)(merge)\n","  merge = Dropout(0.5)(merge)\n","\n","  # real/fake output\n","  out1 = Dense(1, activation='sigmoid', name='valid')(merge)\n","  # class label output\n","  out2 = Dense(n_classes, activation='softmax', name='class')(merge)\n","  # define model\n","  model = Model(in_image, [out1, out2], name=\"Discriminator\")\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cLWWWfE6L20n"},"source":["# define G"]},{"cell_type":"code","metadata":{"id":"feTOreTCFQEr"},"source":["# define the standalone generator model\n","def define_generator(latent_dim=50, n_classes=3, d=D):\n","  #Initailize Weights\n","  init = RandomNormal(stddev=STD)\n","    \n","  #Take in noise as input\n","  in_z = keras.Input(shape=(latent_dim,))\n","  print(f\"Shape of Noise Vector: {in_z.shape}\")\n","  \n","  #Create a dense layer\n","  dense = keras.layers.Dense(200*d, activation=\"relu\", kernel_initializer = init)\n","  \n","  X = dense(in_z)\n","  X = keras.layers.Reshape((200,d))(X)\n","  print(f\"Shape of X: {X.shape}\")\n","\n","  A = keras.layers.Dot(axes=(2, 2))([X,X])\n","  A = keras.backend.expand_dims(A, axis = -1)\n","\n","  A = Activation('tanh')(A)\n","  print(f\"Shape of A: {A.shape}\")\n","  \n","  # define model\n","  model = Model(in_z, A, name=\"Generator\")\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fK7NGsGKL5Ts"},"source":["#define GAN"]},{"cell_type":"code","metadata":{"id":"bzf_8DlyHJGD"},"source":["def all_model(latent_dim=50):\n","  # define D & G\n","  d_model = define_discriminator()\n","  g_model = define_generator(latent_dim)\n","\n","  # compile D\n","  opt = optimizers.Adam(lr=LR_D, beta_1=BETA_D)\n","  d_model.compile(loss=['binary_crossentropy', 'sparse_categorical_crossentropy'], loss_weights=LOSS_WEIGHTS, optimizer=opt, metrics=['acc'])\n","\n","  # define GAN\n","  d_model.trainable = False\n"," \n","  in_noise = keras.Input(shape=(latent_dim,))\n","  img = g_model(in_noise)\n","\n","  valid, label = d_model(img)\n","  gan_model = Model(in_noise, [valid, label], name=\"GAN\")\n","\n","  opt = optimizers.Adam(lr=LR_G, beta_1=BETA_G)\n","  gan_model.compile(loss=['binary_crossentropy', 'sparse_categorical_crossentropy'], loss_weights=LOSS_WEIGHTS, optimizer=opt, metrics=['acc'])\n","\n","  return d_model, g_model, gan_model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B8CEB6VkKsQc"},"source":["d_model, g_model, gan_model = all_model(LATENT_DIM)\n","d_model.summary()\n","g_model.summary()\n","gan_model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zm7qDbXsMJcD"},"source":["#Auxiliary function"]},{"cell_type":"code","metadata":{"id":"gNoZcRz_MOyZ"},"source":["# load images\n","def load_real_samples():\n","  # load dataset\n","  (X, y)= loadDataset()\n","  # expand to 3d, e.g. add channels\n","  X = np.expand_dims(X, axis=-1)\n","  # convert from ints to floats\n","  X = X.astype('float32')\n","  print(X.shape, y.shape)\n","  X_train, X_remain, y_train, y_remain = train_test_split(X, y, test_size=0.2, random_state=42)\n","  X_val, X_test, y_val, y_test = train_test_split(X_remain, y_remain, test_size=0.5, random_state=42)\n","\n","  print(f\"Training Data, X shape: {X_train.shape}, y shape: {y_train.shape}\")\n","  print(f\"Validation Data, X shape: {X_val.shape}, y shape: {y_val.shape}\")\n","  print(f\"Test Data, X shape: {X_test.shape}, y shape: {y_test.shape}\")\n","\n","  return [X_train, y_train],[X_val, y_val],[X_test, y_test]\n","     \n","# select real samples\n","def generate_real_samples(dataset, n_samples):\n","\t# split into images and labels\n","\timages, labels = dataset\n","\t# choose random instances\n","\tix = np.random.randint(0, images.shape[0], n_samples)\n","\t# select images and labels\n","\tX, labels = images[ix], labels[ix] \n","\ty = np.ones((n_samples, 1))              \n","\treturn X, [y, labels]\n","\n","def generate_random_ecodings(n_samples):\n","  enc_idx = np.arange(0,len(encoded_data))\n","  sample_idx = np.random.choice(enc_idx, size = n_samples)\n","  samples = []\n","  labels = []\n","  #print(sample_idx)\n","  for idx in sample_idx:\n","    samples.append(encoded_data[idx][:3])\n","    label = encoded_data[idx][:3]\n","    if label[0]==1:\n","      labels.append(0)\n","    elif label[1]==1:\n","      labels.append(1)\n","    else:\n","      labels.append(2)\n","  return np.array(samples), np.array(labels)\n","\n","# generate points in latent space as input for the generator\n","def generate_latent_points(latent_dim, n_samples, n_classes=3):\n","  #Generate noise, 3 dimensions short of latent_dim\n","  z_noise = np.random.normal(0, 1, size=[n_samples,latent_dim-3])   # Gaussian distribution\n","  #Generate encoding of 3 dimensions\n","  z_encoding, labels = generate_random_ecodings(n_samples)\n","  #Concatenate z_noise and z_encoding to create input of latent_dim\n","  z_input = np.concatenate((z_noise, z_encoding), axis = 1)\n","  return [z_input, labels]\n","\n","# use the generator to generate n fake examples, with class labels\n","def generate_fake_samples(generator, latent_dim, n_samples):\n","  # generate points in latent space\n","  z_input, labels_input = generate_latent_points(latent_dim, n_samples)\n","  # predict outputs\n","  images = generator.predict(z_input)\n","  y = np.zeros((n_samples, 1))           \n","  return images, [y, labels_input]\n","\n","# generate samples and save as a plot and save the model\n","def summarize_performance(step, d_model):\n","  path = 'AC_Brain/mode3'\n","  filename1 = path + '/weights/d_model_%04d.h5' % (step+1)\n","  d_model.save_weights(filename1)\n","  print('>Saved: %s' % filename1)\n","\n","# create a line plot of loss for the gan and save to file\n","def plot_history(train_hist, validation_hist):\n","  path = 'AC_Brain/mode3'\n","  # dr_v_loss1, df_v_loss1, g_v_loss1, dr_v_acc1, df_v_acc1, dr_c_acc1, df_c_acc1 = train_hist\n","  # # plot train_data loss\n","  # pyplot.plot(dr_v_loss1, label='D-validity-real')\n","  # pyplot.plot(df_v_loss1, label='D-validity-fake')\n","  # pyplot.plot(g_v_loss1, label='G-validity')\n","  # pyplot.legend()\n","  # pyplot.savefig(path + '/plot_train_loss.pdf')\n","  # pyplot.close()\n"," \n","  # plot train_datad accuracy\n","  # pyplot.plot(dr_v_acc1, label='validity-real')\n","  # pyplot.plot(df_v_acc1, label='validity-fake')\n","  # pyplot.legend()\n","  # pyplot.savefig(path + '/plot_train_valid_acc.pdf')\n","  # pyplot.close()\n","\n","  # pyplot.plot(dr_c_acc1, label='class-real')\n","  # pyplot.plot(df_c_acc1, label='class-fake')\n","  # pyplot.legend()\n","  # pyplot.savefig(path + '/plot_train_class_acc.pdf')\n","  # pyplot.close()\n"," \n","  dr_v_loss2, df_v_loss2, dr_v_acc2, df_v_acc2, dr_c_acc2 = validation_hist\n","  # # plot validation_data loss\n","  # pyplot.plot(dr_v_loss2, label='validity-real')\n","  # pyplot.plot(df_v_loss2, label='validity-fake')\n","  # pyplot.legend()\n","  # pyplot.savefig(path + '/plot_validation_loss.pdf')\n","  # pyplot.close()\n","\n","  # plot validation_data accuracy\n","  pyplot.plot(dr_v_acc2, label='validity-real')\n","  pyplot.plot(df_v_acc2, label='validity-fake')\n","  pyplot.plot(dr_c_acc2, label='class-real')\n","  pyplot.legend()\n","  pyplot.savefig(path + '/plot_validation_acc.pdf')\n","  pyplot.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i4q7BrEyjTh2"},"source":["#Training"]},{"cell_type":"code","metadata":{"id":"ad0hvdDOjO96"},"source":["# train the generator and discriminator\n","def train(g_model, d_model, gan_model, dataset, val_dataset, n_epochs=300, latent_dim=LATENT_DIM, n_batch=BATCH_SIZE):\n","  epoch=0\n","  # calculate the number of batches per training epoch\n","  bat_per_epo = int(dataset[0].shape[0] / n_batch)\n","  # calculate the number of training iterations\n","  n_steps = bat_per_epo * n_epochs\n","  # calculate the real/fake batch_size\n","  half_batch = int(n_batch / 2)\n","  # prepare lists for train_data hist\n","  dr_v_loss1, df_v_loss1, g_v_loss1, dr_v_acc1, df_v_acc1, dr_c_acc1, df_c_acc1 = list(), list(), list(), list(), list(), list(), list()\n","  # prepare lists for validation_data hist\n","  dr_v_loss2, df_v_loss2, dr_v_acc2, df_v_acc2, dr_c_acc2 = list(), list(), list(), list(), list()\n","\n","  # manually enumerate epochs\n","  for i in range(n_steps):\n","    #----------------------------------------\n","    # update discriminator model weights\n","    #----------------------------------------\n","\n","    # get randomly selected 'real' samples\n","    X_real, [y_real, labels_real] = generate_real_samples(dataset, half_batch)\n","    dr_metrics = d_model.train_on_batch(X_real, [y_real, labels_real])\n","    # generate 'fake' \n","    X_fake, [y_fake, labels_fake] = generate_fake_samples(g_model, latent_dim, half_batch)\n","    df_metrics = d_model.train_on_batch(X_fake, [y_fake, labels_fake])\n","\n","    # summarize the loss and accuracy\n","    d_metrics = 0.5 * np.add(dr_metrics, df_metrics)\n","\n","    #----------------------------------------\n","    # update the generator via the discriminator's error\n","    #----------------------------------------\n","\n","    # prepare points in latent space as input for the generator\n","    [z_input, z_labels] = generate_latent_points(latent_dim, n_batch)   \n","    y_gan = np.ones((n_batch, 1)) \n","    g_metrics = gan_model.train_on_batch(z_input, [y_gan, z_labels])\n","\n","    # summarize loss on this batch\n","    print('STEP:%d, D{v_l: %.3f, v_acc: [%.1f| %.1f| %.1f], c_acc: [%.1f| %.1f]}  G{v_l: %.3f, v_acc: %.1f, c_acc: %.1f}'\n","        % (i+1, d_metrics[1], 100*dr_metrics[3], 100*df_metrics[3], 100*d_metrics[3], 100*dr_metrics[4], 100*df_metrics[4], \n","          g_metrics[1], 100*g_metrics[3], 100*g_metrics[4]))\n","    # metrics[0]: loss, metrics[1]: validity_loss, metrics[2]: classification_loss, metrics[3]: validity_accuracy, metrics[4]: classification_accuracy\n","\n","    # record history\n","    dr_v_loss1.append(dr_metrics[1])\n","    df_v_loss1.append(df_metrics[1])\n","    g_v_loss1.append(g_metrics[1])\n","    dr_v_acc1.append(dr_metrics[3])\n","    df_v_acc1.append(df_metrics[3])\n","    dr_c_acc1.append(dr_metrics[4])\n","    df_c_acc1.append(df_metrics[4])\n","\n","    #----------------------------------------\n","    # evaluation\n","    #----------------------------------------\n","    if (i+1) % (bat_per_epo) == 0:\n","      epoch+=1\n","      # generate real validation data\n","      X_r_val, labels_r_val = val_dataset\n","      num_test = X_r_val.shape[0]\n","      # generate fake validation data\n","      y_r_val = ones((num_test, 1))\n","      X_f_val, [y_f_val, labels_f_val] = generate_fake_samples(g_model, latent_dim, num_test)\n","\n","      print(f\"\\nValidation Metrics of Discriminator:\")\n","      # evaluate both real and fake valid_dataset\n","      valid_metrics_r = d_model.evaluate(X_r_val, [y_r_val, labels_r_val], verbose=1)\n","      valid_metrics_f = d_model.evaluate(X_f_val, [y_f_val, labels_f_val], verbose=1)\n","\n","      v_acc = 50 * (valid_metrics_r[3] + valid_metrics_f[3])  \n","      three_c_acc = 100 * valid_metrics_r[4]   \n","\n","      # two class accuracy\n","      _, labels_pred = d_model.predict(X_r_val)\n","      labels_pred = np.argmax(labels_pred, axis=1)\n","      # print('val {class_0: %d, class_1: %d, class_2: %d}' % (np.sum(labels_r_val==0), np.sum(labels_r_val==1), np.sum(labels_r_val==2)))\n","      # print('pred {class_0: %d, class_1: %d, class_2: %d}' % (np.sum(labels_pred==0), np.sum(labels_pred==1), np.sum(labels_pred==2)))\n","      labels_2_val, labels_2_pred = labels_r_val.copy(), labels_pred.copy()\n","      # classify 2 as 1\n","      labels_2_val[labels_2_val==2] = 1\n","      labels_2_pred[labels_2_pred==2] = 1\n","      # calculate the accuracy \n","      correct = np.sum(labels_2_val==labels_2_pred)\n","      two_c_acc = correct / num_test * 100\n","           \n","      print('average v_acc: %.3f, three class acc: %.3f, two class acc: %.3f' % (v_acc, three_c_acc, two_c_acc))\n","      print(\"=\"*100)\n","      # save good models\n","      if three_c_acc > 64 or two_c_acc > 67:\n","      # if three_c_acc > 65:\n","        summarize_performance(i, d_model)\n","\n","      # record history\n","      dr_v_loss2.append(valid_metrics_r[1])\n","      df_v_loss2.append(valid_metrics_f[1])\n","      dr_v_acc2.append(valid_metrics_r[3])\n","      df_v_acc2.append(valid_metrics_f[3])\n","      dr_c_acc2.append(valid_metrics_r[4])\n","\n","    # print epoch\n","    if (i+1) % (bat_per_epo * 10) == 0:\n","      print(f\"Epoch: {epoch}\")\n","      print(\"=\"*100)\n","   \n","      # plot history\n","      train_hist = [dr_v_loss1, df_v_loss1, g_v_loss1, dr_v_acc1, df_v_acc1, dr_c_acc1, df_c_acc1]\n","      validation_hist = [dr_v_loss2, df_v_loss2, dr_v_acc2, df_v_acc2, dr_c_acc2]\n","      plot_history(train_hist, validation_hist)\n","      point = [dr_v_acc2, df_v_acc2, dr_c_acc2]\n","      np.save('AC_Brain/mode3/point_3', point)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FvIIOPHvqBse"},"source":["# load image data\n","train_data, val_data, test_data = load_real_samples()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d7SzbdH3qCS2"},"source":["epochs = 100\n","\n","# define model\n","discriminator, generator, gan_model = all_model(LATENT_DIM)\n","# train model\n","train(generator, discriminator, gan_model, train_data, val_data, n_epochs=epochs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"puZYjV3h8JJ3"},"source":["pathway = '/weights/d_model_0720.h5'\n","\n","test(LATENT_DIM, test_data, pathway)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZuSOHFh7PwIK"},"source":["# Test\n"]},{"cell_type":"code","metadata":{"id":"0MjkQsg3Pxuy"},"source":["from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","from sklearn import metrics\n","\n","def calculate_score(test, pred):\n","  new_pred = np.zeros((pred.shape[0], 2))\n","  new_pred[:, 0] = pred[:, 0]\n","  new_pred[:, 1] = pred[:, 1] + pred[: ,2]\n","  score = new_pred[:, 1]\n","  return score\n","\n","def test(latent_dim, test_dataset, pathway):\n","  # load model\n","  path = 'AC_Brain/mode3'\n","\n","  d_model, _, _ = all_model(LATENT_DIM)\n","  d_model.load_weights(path + pathway)\n","\n","  X_test, labels_test = test_dataset\n","  num_test = X_test.shape[0]\n","  y_test = ones((num_test, 1))\n","\n","  print(f\"\\nValidation Metrics of Discriminator:\")\n","  test_metrics = d_model.evaluate(X_test, [y_test, labels_test], verbose=1)\n","  v_acc = 100 * test_metrics[3]\n","  three_c_acc = 100 * test_metrics[4]\n","\n","  # two class accuracy\n","  _, temp_pred = d_model.predict(X_test)\n","  labels_pred = np.argmax(temp_pred, axis=1)\n","\n","  correct = np.sum(labels_pred==labels_test)\n","  acc = correct / num_test * 100\n","  print('test: %.3f' % acc)\n","\n","  labels_2_test, labels_2_pred = labels_test.copy(), labels_pred.copy()\n","  # classify 2 as 1\n","  labels_2_test[labels_2_test==2] = 1\n","  labels_2_pred[labels_2_pred==2] = 1\n","  # calculate the accuracy\n","  correct = np.sum(labels_2_test==labels_2_pred)\n","  two_c_acc = correct / num_test * 100\n","  print('average v_acc: %.3f, three class acc: %.3f, two class acc: %.3f' % (v_acc, three_c_acc, two_c_acc))\n","  print(\"=\"*100)\n","\n","  # three class confusion metrics\n","  target_names = ['class 0', 'class 1', 'class 2']\n","  print('three class:')\n","\n","  # calculate precision, recall, f1 score\n","  print(classification_report(labels_test, labels_pred, target_names=target_names))\n","\n","  # calculate AUC\n","  onehot = to_categorical(labels_test, num_classes=3)\n","  AUC = metrics.roc_auc_score(onehot, temp_pred, multi_class='ovr')\n","  print('AUC: ', AUC)\n","  print(\"=\"*100)\n","\n","  # two class confusion metrics\n","  target_names = ['class 0', 'class 1']\n","  print('two class:')\n","  # calculate precision, recall, f1 score\n","  print(classification_report(labels_2_test, labels_2_pred, target_names=target_names))\n","\n","  # calcuate specificity\n","  tn, fp, fn, tp = confusion_matrix(labels_2_test, labels_2_pred).ravel()\n","  specificity = tn / (tn+fp)\n","  print('specificity:', specificity)\n","\n","  # calculate AUC\n","  score = calculate_score(labels_2_test, temp_pred)\n","  AUC = metrics.roc_auc_score(labels_2_test, score)\n","  print('AUC: ', AUC)\n","  print(\"=\"*100)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-4eKDxvXQYkH"},"source":["# load data\n","train_data, val_data, test_data = load_real_samples()\n","\n","test(LATENT_DIM, test_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Po9vtKuWxv1"},"source":[""],"execution_count":null,"outputs":[]}]}