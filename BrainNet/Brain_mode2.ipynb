{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Brain_mode2.ipynb","provenance":[],"collapsed_sections":["9UIlph7YzICh","NWu7gtY4PPv4","vkPKyA_EnJus","Qtzgq8Anm_Qc","UyIRcJy2hxMX","X70XqBemjGDD"],"authorship_tag":"ABX9TyOpzpnXwT2mtjv57e2/po0G"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"P3sT-TmflycC"},"source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","path = \"/content/drive/My Drive/GAN_for_Neural_Graph\"\n","\n","os.chdir(path)\n","os.listdir(path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9UIlph7YzICh"},"source":["#Import Libaries"]},{"cell_type":"code","metadata":{"id":"bhvNOgejl0X4"},"source":["import pandas as pd\n","import os\n","import numpy as np\n","from numpy import expand_dims\n","from keras import backend as K\n","from keras import activations\n","from keras import initializers\n","from keras import regularizers\n","from keras import constraints\n","from keras import optimizers\n","from keras import callbacks\n","from keras.engine import Layer\n","from keras.engine import InputSpec\n","from keras.models import Sequential\n","from keras.models import Model\n","from keras.layers import Input\n","from keras.layers import Dense\n","from keras.layers import Reshape\n","from keras.layers import Flatten\n","from keras.layers import Conv2D\n","from keras.layers import Conv2DTranspose\n","from keras.layers import LeakyReLU\n","from keras.layers import BatchNormalization\n","from keras.layers import Dropout\n","from keras.layers import Embedding\n","from keras.layers import Activation\n","from keras.layers import Concatenate\n","from keras.layers import Add\n","from keras.utils import to_categorical\n","from keras.utils import conv_utils\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","from keras.initializers import RandomNormal\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import shuffle\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import MultiLabelBinarizer"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NWu7gtY4PPv4"},"source":["# Load data"]},{"cell_type":"code","metadata":{"id":"Z0BXBkCaPMA6"},"source":["table = pd.read_csv(\"./Dataset/left_table.csv\")\n","table1 = table.loc[:,['Id','label_id']]\n","print(table1.head())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ki1TORdPPTUL"},"source":["table_dict = dict(zip(table1.Id, table1.label_id))\n","# print(table_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LCQk4T7dPTxT"},"source":["csv_files = os.listdir(\"./Dataset/FC_norm\")\n","csv_files = [file for file in csv_files if file[-1]=='v']\n","print(len(csv_files))\n","\n","X = []\n","y = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AYu-kCMwPU3Y"},"source":["for file in csv_files:\n","  df = pd.read_csv(os.path.join(\"./Dataset/FC_norm\",file),header = None)\n","  X.append(df.to_numpy())\n","  key = int(file.strip('.csv'))\n","#     print(key)\n","  y.append(int(table_dict[key]))\n","\n","X = np.array(X)\n","print(X.shape)\n","y = np.array(y)\n","print(y.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"byuPjScNbvzV"},"source":["table2 = table.loc[:,['label_id', 'Age', 'Gender']]\n","print(table2.head())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DcwCIZhRb1zi"},"source":["#Normalize Age column\n","table2['Age'] = (table2['Age'] - table2['Age'].min()) / (table2['Age'].max() - table2['Age'].min())\n","\n","#Make Gender 0, 1 instead of 1, 0\n","table2['Gender'] = table2['Gender'].replace([2],0)\n","print(table2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WvBAL08nM56p"},"source":["label = np.array(list(table2['label_id']))\n","label = np.expand_dims(label, axis=1)\n","age = np.array(list(table2['Age']))\n","age = np.expand_dims(age, axis=1)\n","gender = np.array(list(table2['Gender']))\n","gender = np.expand_dims(gender, axis=1)\n","\n","combine = np.concatenate([label, age, gender], axis=1)\n","print(combine.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xhx-vEKbM7n5"},"source":["def loadDataset():\n","  return X, combine"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HElWROJyPcE2"},"source":["#One-hot encose the class labels\n","class_labels = np.array(list(table2['label_id']))\n","class_labels = np.expand_dims(class_labels, axis=1)\n","\n","mlb = MultiLabelBinarizer()\n","class_labels = mlb.fit_transform(class_labels)\n","print(class_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7MkVEUVqPdYr"},"source":["encoded_data = []\n","for label, row in zip(class_labels, table2.itertuples()):\n","    label = list(label)\n","    age = [row.Age]\n","    gender = [row.Gender]\n","#     print(type(label), type(age), type(gender))\n","#     print(label, age, gender)\n","    encoded_data.append(label+age+gender)\n","    #print(label+age+gender)\n","\n","# encoded_data = np.array(encoded_data)\n","for i in range(2):\n","    print(encoded_data[i])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8l5vh04wPept"},"source":["enc_idx = np.arange(0,len(encoded_data))\n","\n","sample_idx = np.random.choice(enc_idx, size = 10)\n","\n","print(sample_idx)\n","\n","for idx in sample_idx:\n","    print(encoded_data[idx])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lmMlZCnoPf2d"},"source":["sample_idx = np.random.choice(len(encoded_data), size = 10)\n","print(sample_idx)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gNoZcRz_MOyZ"},"source":["# load images\n","def load_real_samples():\n","  # load dataset\n","  (X, combine)= loadDataset()\n","  # expand to 3d, e.g. add channels\n","  X = np.expand_dims(X, axis=-1)\n","  # convert from ints to floats\n","  X = X.astype('float32')\n","  print(X.shape, combine.shape)\n","  X_train, X_remain, combine_train, combine_remain = train_test_split(X, combine, test_size=0.2, random_state=42)  # combine  sex age\n","  X_val, X_test, combine_val, combine_test = train_test_split(X_remain, combine_remain, test_size=0.5, random_state=42)\n","\n","  # seperate label, age, gender\n","  y_train = combine_train[:, 0]\n","  y_test = combine_test[:, 0]\n","  y_val = combine_val[:, 0]\n","  as_train = combine_train[:, 1:] \n","  as_test = combine_test[:, 1:]  \n","  as_val = combine_val[:, 1:]\n","\n","  print(f\"Training Data, X shape: {X_train.shape}, y shape: {y_train.shape}, as shape: {as_train.shape}\")\n","  print(f\"Validation Data, X shape: {X_val.shape}, y shape: {y_val.shape}, as shape: {as_val.shape}\")\n","  print(f\"Test Data, X shape: {X_test.shape}, y shape: {y_test.shape}, as shape: {as_test.shape}\")\n","\n","  output = [X_train, y_train, as_train[:, 1]],[X_val, y_val, as_val[:, 1]],[X_test, y_test, as_test[:, 1]]\n","  print(f\"Code_train shape: {as_train[:, 1].shape}\")\n","\n","  return output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vkPKyA_EnJus"},"source":["#Hyper parameters"]},{"cell_type":"code","metadata":{"id":"0Wxk5EiwnL6G"},"source":["DROPOUT = 0.5\n","MOMENTUM = 0.9\n","LR = 0.001  \n","DECAY = 0.0005\n","ALPHA = 0.33\n","FE_CHANNEL = 64\n","CODE_CHANNEL = 8\n","MERGE_CHANNEL = 32\n","BATCH_SIZE = 32\n","\n","'''\n","The number of E2E layers\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qtzgq8Anm_Qc"},"source":["#Build Model"]},{"cell_type":"code","metadata":{"id":"GQnmpA-znDOP"},"source":["class E2E_conv(Layer):\n","  def __init__(self, rank,\n","         filters,\n","         kernel_size,\n","         strides=1,\n","         padding='valid',\n","         activation=None,\n","         kernel_initializer='glorot_uniform',\n","         kernel_regularizer=None,\n","         kernel_constraint=None,\n","         **kwargs):\n","    super(E2E_conv, self).__init__(**kwargs)\n","    self.rank = rank\n","    self.filters = filters\n","    self.kernel_size = conv_utils.normalize_tuple(kernel_size, rank, 'kernel_size')\n","    self.strides = conv_utils.normalize_tuple(strides, rank, 'strides')\n","    self.padding = conv_utils.normalize_padding(padding)\n","    self.activation = activations.get(activation)\n","    self.kernel_initializer = initializers.get(kernel_initializer)\n","    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n","    self.kernel_constraint = constraints.get(kernel_constraint)\n","    self.input_spec = InputSpec(ndim=self.rank + 2)\n","\n","  def build(self, input_shape):\n","    channel_axis = -1\n","    if input_shape[channel_axis] is None:\n","      raise ValueError('The channel dimension of the inputs'\n","               'should be defined. Found `None`.')\n","    input_dim = input_shape[channel_axis]\n","    kernel_shape = self.kernel_size + (input_dim, self.filters)\n","\n","    self.kernel = self.add_weight(shape=kernel_shape,\n","                    initializer=self.kernel_initializer,\n","                    name='kernel',\n","                    regularizer=self.kernel_regularizer,\n","                    constraint=self.kernel_constraint)\n","    \n","    # Set input spec.\n","    self.input_spec = InputSpec(ndim=self.rank + 2,\n","                   axes={channel_axis:input_dim})\n","    self.built = True\n","\n","  def call(self, inputs):\n","    kernel_shape = K.get_value(self.kernel).shape\n","    d = kernel_shape[1]\n","    kernellxd = K.reshape(self.kernel[0,:], (1, kernel_shape[1], kernel_shape[2], kernel_shape[3]))  # row vector\n","    kerneldxl = K.reshape(self.kernel[1,:], (kernel_shape[1], 1, kernel_shape[2], kernel_shape[3]))  # column vector\n","    convlxd = K.conv2d(\n","        inputs,\n","        kernellxd,\n","        strides=self.strides,\n","        padding=self.padding)\n","    convdxl = K.conv2d(\n","        inputs,\n","        kerneldxl,\n","        strides=self.strides,\n","        padding=self.padding)\n","    concat1 = K.concatenate([convdxl]*d, axis=1)\n","    concat2 = K.concatenate([convlxd]*d, axis=2)\n","    return concat1 + concat2\n","\n","  def compute_output_shape(self, input_shape):\n","    return (input_shape[0], input_shape[1], input_shape[2], self.filters)\n","\n","  def get_config(self):\n","    config = {\n","        'rank': self.rank,\n","        'filters': self.filters,\n","        'kernel_size': self.kernel_size,\n","        'strides': self.strides,\n","        'padding': self.padding,\n","        'activation': activations.serialize(self.activation),\n","        'kernel_initializer': initializers.serialize(self.kernel_initializer),\n","        'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n","        'kernel_constraint': constraints.serialize(self.kernel_constraint)\n","    }\n","    base_config = super(E2E_conv, self).get_config()\n","    return dict(list(base_config.items()) + list(config.items()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B7G8B4M4oO6x"},"source":["def define_E2E(in_shape=(200,200,1), n_classes=3):\n","  # Setting l2_norm regularizer\n","  reg = regularizers.l2(DECAY)\n","  # kernel initialization\n","  kernel_init = initializers.he_uniform()\n","  in_image = Input(shape=in_shape)\n","\n","  # E2E layer\n","  fe = E2E_conv(2, 16, (2, 200), kernel_regularizer=reg)(in_image) \n","  fe = LeakyReLU(alpha=ALPHA)(fe)\n","\n","  fe = E2E_conv(2, 32, (2, 200), kernel_regularizer=reg)(fe) \n","  fe = LeakyReLU(alpha=ALPHA)(fe)\n","\n","  # E2N layer\n","  temp1 = Conv2D(64, (1, 200), kernel_regularizer=reg, name='row')(fe)   \n","  temp2 = Conv2D(64, (200, 1), kernel_regularizer=reg, name='column')(fe) \n","  temp2 = Reshape((200, 1, 64))(temp2)\n","  fe = Add()([temp1, temp2])                    \n","  fe = LeakyReLU(alpha=ALPHA)(fe)\n","\n","  # N2G layer\n","  fe = Conv2D(128, (200, 1), kernel_regularizer=reg)(fe)\n","  fe = LeakyReLU(alpha=ALPHA)(fe)\n","\n","  # flatten feature maps\n","  fe = Flatten()(fe)\n","\n","  fe = Dense(FE_CHANNEL, kernel_regularizer=reg, kernel_initializer=kernel_init)(fe)\n","  fe = LeakyReLU(alpha=ALPHA)(fe)\n","  fe = Dropout(DROPOUT)(fe)\n","\n","  # code input\n","  code_shape = (1,)  \n","  in_code = Input(shape=code_shape, name='in_code')\n","\n","  code = Dense(CODE_CHANNEL, kernel_regularizer=reg, kernel_initializer=kernel_init)(in_code)\n","  code = LeakyReLU(alpha=ALPHA)(code)\n","  code = Dropout(DROPOUT)(code)\n","\n","  # concatenate image and code\n","  merge = Concatenate()([fe, code])\n","\n","  # # concatenate image and code\n","  # merge = Concatenate()([fe, in_code])\n","\n","  merge = Dense(MERGE_CHANNEL, kernel_regularizer=reg, kernel_initializer=kernel_init)(merge)\n","  merge = LeakyReLU(alpha=ALPHA)(merge)\n","  merge = Dropout(DROPOUT)(merge)\n","\n","  out = Dense(n_classes, activation='softmax', name='out')(merge)\n","    \n","  # define model\n","  model = Model([in_image, in_code], out)\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fuQssoINq2Jd"},"source":["BrainNetCNN = define_E2E()\n","BrainNetCNN.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UyIRcJy2hxMX"},"source":["# Training"]},{"cell_type":"code","metadata":{"id":"G_0fg3Dhf_xZ"},"source":["# load data\n","train_data, val_data, test_data = load_real_samples()\n","X_train, y_train, code_train = train_data\n","X_val, y_val, code_val = val_data\n","X_test, y_test, code_test = test_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9m8iJp4eq3Cd"},"source":["path = \"./BrainNetCNN/mode2/epoch_30.h5\"\n","batch_size = BATCH_SIZE\n","\n","opt = optimizers.SGD(momentum=MOMENTUM,nesterov=True,lr=LR)\n","checkpoint = ModelCheckpoint(path, monitor='val_acc', verbose=0, save_best_only=True)\n","earlystopping = EarlyStopping(patience=10)\n","callbacks_list = [checkpoint,earlystopping]\n","\n","# define model\n","model_1 = define_E2E()\n","model_1.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['acc'])\n","\n","\n","\n","# train\n","history = model_1.fit([X_train, code_train], y_train,\n","              epochs=30,\n","              batch_size=batch_size,\n","              validation_data=([X_val, code_val], y_val),\n","              callbacks=callbacks_list,\n","              shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zh38Vf_nrYKk"},"source":["import matplotlib.pyplot as plt\n","\n","def plot_learning_curves(history,epoch,min_val,max_val):\n","        pd.DataFrame(history.history).plot(figsize=(8,5))\n","        plt.grid(True)\n","        plt.axis([0, epoch, min_val, max_val])\n","        plt.show()\n","plot_learning_curves(history,5,0,2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X70XqBemjGDD"},"source":["# Test"]},{"cell_type":"code","metadata":{"id":"xPBbaCfhe6XB"},"source":["from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","from sklearn import metrics\n","\n","def calculate_score(test, pred):\n","  new_pred = np.zeros((pred.shape[0], 2))\n","  new_pred[:, 0] = pred[:, 0]\n","  new_pred[:, 1] = pred[:, 1] + pred[: ,2]\n","  score = new_pred[:, 1]\n","  return score\n","\n","def test(test_dataset):\n","  # load model\n","  path = \"./BrainNetCNN/mode2/epoch_30.h5\"\n","  model_1 = define_E2E()\n","  opt = optimizers.SGD(momentum=MOMENTUM,nesterov=True,lr=LR)\n","  model_1.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['acc'])\n","  model_1.load_weights(path)\n","\n","  X_test, labels_test, code_test = test_dataset\n","  num_test = X_test.shape[0]\n","\n","  print(f\"\\nValidation Metrics of Discriminator:\")\n","  test_metrics = model_1.evaluate([X_test, code_test], labels_test, verbose=1)\n","  three_c_acc = 100 * test_metrics[1]\n","\n","  # two class accuracy\n","  temp_pred = model_1.predict([X_test, code_test])\n","  labels_pred = np.argmax(temp_pred, axis=1)\n","\n","  correct = np.sum(labels_pred==labels_test)\n","  acc = correct / num_test * 100\n","  print('test: %.3f' % acc)\n","\n","  labels_2_test, labels_2_pred = labels_test.copy(), labels_pred.copy()\n","  # classify 2 as 1\n","  labels_2_test[labels_2_test==2] = 1\n","  labels_2_pred[labels_2_pred==2] = 1\n","  # calculate the accuracy\n","  correct = np.sum(labels_2_test==labels_2_pred)\n","  two_c_acc = correct / num_test * 100\n","  print('three class acc: %.3f, two class acc: %.3f' % (three_c_acc, two_c_acc))\n","  print(\"=\"*100)\n","\n","  # three class confusion metrics\n","  target_names = ['class 0', 'class 1', 'class 2']\n","  print('three class:')\n","\n","  # calculate precision, recall, f1 score\n","  print(classification_report(labels_test, labels_pred, target_names=target_names, digits=4))\n","\n","  # calculate AUC\n","  onehot = to_categorical(labels_test, num_classes=3)\n","  AUC = metrics.roc_auc_score(onehot, temp_pred, multi_class='ovr')\n","  print('AUC: ', AUC)\n","  print(\"=\"*100)\n","\n","  # two class confusion metrics\n","  target_names = ['class 0', 'class 1']\n","  print('two class:')\n","  # calculate precision, recall, f1 score\n","  print(classification_report(labels_2_test, labels_2_pred, target_names=target_names, digits=4))\n","\n","  # calcuate specificity\n","  tn, fp, fn, tp = confusion_matrix(labels_2_test, labels_2_pred).ravel()\n","  specificity = tn / (tn+fp)\n","  print('specificity:', specificity)\n","\n","  # calculate AUC\n","  score = calculate_score(labels_2_test, temp_pred)\n","  AUC = metrics.roc_auc_score(labels_2_test, score)\n","  print('AUC: ', AUC)\n","  print(\"=\"*100)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8DhuhxEeiX9X"},"source":["test(test_data)"],"execution_count":null,"outputs":[]}]}