{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Logistic_regression&SVM.ipynb","provenance":[],"collapsed_sections":["pMaTL35VjccH","hxXxHikRLsYy","Cb4FrREQL-rs","ZOuVbAMTMFLf","0o_gq800MJRL","KcSngtV_MPJL"],"authorship_tag":"ABX9TyNrOsH2226Sx6Y0tpV+Pysf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"K6R3Kl0XgYR6"},"source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","path = \"/content/drive/My Drive/GAN_for_Neural_Graph/baeslines\"\n","\n","os.chdir(path)\n","os.listdir(path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zPX1aWuohifr"},"source":["#Config"]},{"cell_type":"code","metadata":{"id":"4dxweisxgZmn"},"source":["DATA_dir = \"Dataset\"\n","left_table_file = 'left_table.csv'\n","matrices_dir = 'FC_norm'\n","pickle_path = \"ABIDE.p\"\n","upsampled_pickle_path = \"ABIDE_upsampled.p\"\n","json_path = \"split_ids.json\"\n","\n","weight_threshold = 0.1\n","train_size = 700\n","val_size = 183"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hZYmcHCchkLS"},"source":["#Process Data"]},{"cell_type":"code","metadata":{"id":"LhQjtcW3hh0b"},"source":["import os, sys\n","from tqdm import tqdm\n","import pickle\n","import json\n","import pandas as pd\n","import numpy as np\n","from sklearn.utils import shuffle\n","from keras.utils import to_categorical\n","\n","# np.set_printoptions(threshold=sys.maxsize)\n","LABEL_LIST = [\"Control\", \"Autism\", \"Aspergers\"]\n","INS_LIST = ['YALE', 'KKI', 'UCLA_1', 'UCLA_2', 'PITT', 'OLIN', 'SDSU', 'TRINITY', 'UM_1', 'UM_2', 'USM', 'CMU', 'LEUVEN_1', 'LEUVEN_2', 'NYU', 'MAX_MUN', 'CALTECH', 'SBL']\n","GENDER_LIST = [1, 2]\n","\n","\n","def map_to_onehot(value, all_values):\n","    \"\"\"\n","    Convert left table entry to one hot vector.\n","    value: a table entry, i.e., the content in a cell of the table. E.g. \"Autism\" in LABEL_LIST\n","    all_values: all possible values that could appear in a column. E.g. LABEL_LIST\n","    \"\"\"\n","    idx = all_values.index(value)\n","    l = len(all_values)\n","    one_hot = [0.,] * idx + [1.,] + [0.] * (l - idx - 1)\n","    return one_hot\n","\n","def preprocess(A, threshold):\n","    \"\"\"\n","    Preprocess an input matrix.\n","    A: adjacency matrix\n","    threshold: convert edge weights to connectivity, i.e., when edge weight is\n","            larger than the threshold, the edge is considered to be connecting\n","            its two corresponding nodes.\n","    \"\"\"\n","    X = A.copy()\n","    # A[X <= threshold] = 0.\n","    # A[X > threshold] = 1.\n","    return A, X\n","\n","def get_backbone_graph(graphs, threshold):\n","    \"\"\"\n","    Get the backbone graph base on graphs from the training set\n","    \"\"\"\n","    A, _ = graphs[:, 0, :, :], graphs[:, 1, :, :]\n","    A_mean = np.mean(A, axis = 0)\n","    A_backbone = A_mean.copy()\n","    A_backbone[A_mean > threshold] = 1.\n","    A_backbone[A_mean <= threshold] = 0.\n","    return A_backbone\n","\n","def set_backbone_graph(graphs, A_backbone):\n","    _, X = graphs[:, 0, :, :], graphs[:, 1, :, :]\n","    # A_backbone = np.mean(A, axis = 0)\n","    N = X.shape[0]\n","    A = np.tile( A_backbone[np.newaxis, :, :], (N, 1, 1) )\n","    graphs[:, 0, :, :] = A\n","    return graphs\n","\n","def convert_to_model_input(graphs):\n","    \"\"\"\n","    Split input graph tensor into a tensor for A and another tensor for X\n","    \"\"\"\n","    A, X = graphs[:, 0, :, :], graphs[:, 1, :, :]\n","    return A, X\n","\n","def load_data(data_root_directory=DATA_dir, left_table_file=left_table_file, matrix_directory=matrices_dir):\n","    \"\"\"\n","    Load data from files that are generated by converter.m\n","    data_root_directory: the root directory where all data files reside\n","    left_table_file: the file name of the left half of the original table\n","    matrix_directory: the directory which contains all csv files of matrices,\n","                file names are the Id entries of their corresponding rows\n","    \"\"\"\n","    left_table = pd.read_csv(os.path.join(data_root_directory, left_table_file))\n","    print(\"Left table of shape\", left_table.shape, \"has been loaded!\")\n","    print(\"Loading graphs...\")\n","    matrices = []\n","    labels = []\n","    genders = []\n","    ages = []\n","    ids = []\n","    for row in tqdm(range(left_table.shape[0])):\n","        id = str(left_table.loc[row, 'Id'])\n","        ids.append(id)\n","        # Read left table\n","        genders.append(map_to_onehot(left_table.loc[row, 'Gender'], GENDER_LIST))\n","        ages.append(float(left_table.loc[row, 'Age']))\n","        # Read adjacency matrix\n","        mtx_path = os.path.join(data_root_directory, matrix_directory, id + \".csv\")\n","        A = np.loadtxt(open(os.path.join(data_root_directory, matrix_directory, id + \".csv\"), \"r\"), delimiter=\",\", skiprows=0)\n","        matrices.append(preprocess(A, weight_threshold))\n","    \n","    # get label\n","    table_l = left_table.loc[:, 'label_id']\n","    for l in table_l:\n","      labels.append(l)\n","    labels = to_categorical(labels, 3)\n","\n","    # result\n","    input_ids = np.array(ids)\n","    input_graphs = np.array(matrices)\n","    input_genders = np.array(genders)\n","    input_ages = np.array(ages)\n","    input_ages /= 100.\n","    input_Y = np.array(labels)\n","\n","    return input_ids, input_graphs, input_genders, input_ages, input_Y\n","\n","def shuffle_data(input_ids, input_graphs, input_genders, input_ages, input_Y):\n","    input_ids, input_graphs, input_genders, input_ages, input_Y = shuffle(input_ids, input_graphs, input_genders, input_ages, input_Y)\n","\n","    split_ids = {'train': list(input_ids[:train_size]),\n","                 'val': list(input_ids[train_size:train_size + val_size]),\n","                 'test': list(input_ids[train_size + val_size:])}\n","\n","    train_graphs, val_graphs, test_graphs = \\\n","                    input_graphs[:train_size, :, :], \\\n","                    input_graphs[train_size:train_size + val_size, :, :], \\\n","                    input_graphs[train_size + val_size:, :, :]\n","\n","    train_genders, val_genders, test_genders = \\\n","                    input_genders[:train_size, :], \\\n","                    input_genders[train_size:train_size + val_size, :], \\\n","                    input_genders[train_size + val_size:, :]\n","\n","    train_ages, val_ages, test_ages = \\\n","                    input_ages[:train_size], \\\n","                    input_ages[train_size:train_size + val_size], \\\n","                    input_ages[train_size + val_size:]\n","\n","    train_Y, val_Y, test_Y = \\\n","                    input_Y[:train_size, :], \\\n","                    input_Y[train_size:train_size + val_size, :], \\\n","                    input_Y[train_size + val_size:, :]\n","\n","    return  train_graphs, val_graphs, test_graphs, \\\n","            train_genders, val_genders, test_genders, \\\n","            train_ages, val_ages, test_ages, \\\n","            train_Y, val_Y, test_Y, \\\n","            split_ids\n","\n","\n","\n","input_ids, input_graphs, input_genders, input_ages, input_Y = load_data()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pMaTL35VjccH"},"source":["# Shuffle and Save"]},{"cell_type":"code","metadata":{"id":"pZZX7pS6SH9U"},"source":["# del json\n","import json\n","\n","train_graphs, val_graphs, test_graphs, \\\n","        train_genders, val_genders, test_genders, \\\n","        train_ages, val_ages, test_ages, \\\n","        train_Y, val_Y, test_Y, split_ids = shuffle_data(input_ids, input_graphs, input_genders, input_ages, input_Y)\n","\n","A_backbone = get_backbone_graph(train_graphs, weight_threshold)\n","train_graphs = set_backbone_graph(train_graphs, A_backbone)\n","val_graphs = set_backbone_graph(val_graphs, A_backbone)\n","test_graphs = set_backbone_graph(test_graphs, A_backbone)\n","\n","datasets = train_graphs, val_graphs, test_graphs, \\\n","        train_genders, val_genders, test_genders, \\\n","        train_ages, val_ages, test_ages, \\\n","        train_Y, val_Y, test_Y\n","\n","pickle.dump( datasets, open( pickle_path, \"wb\" ) )\n","json = json.dumps(split_ids)\n","with open(json_path, \"w\") as file:\n","    file.write(json)\n","\n","train_graphs, val_graphs, test_graphs, \\\n","        train_genders, val_genders, test_genders, \\\n","        train_ages, val_ages, test_ages, \\\n","        train_Y, val_Y, test_Y = pickle.load( open( pickle_path, \"rb\" ) )\n","print(\"[Training]   Graph shape, Gender shape, Ages shape, Y shape: \\n\\t\", \\\n","    train_graphs.shape, train_genders.shape, train_ages.shape, train_Y.shape)\n","print(\"[Validation] Graph shape, Gender shape, Ages shape, Y shape: \\n\\t\", \\\n","    val_graphs.shape, val_genders.shape, val_ages.shape, val_Y.shape)\n","print(\"[Test]       Graph shape, Gender shape, Ages shape, Y shape: \\n\\t\", \\\n","    test_graphs.shape, test_genders.shape, test_ages.shape, test_Y.shape)\n","\n","print(\"[Training]   Class distribution\", np.sum(train_Y, axis = 0))\n","print(\"[Validation] Class distribution\", np.sum(val_Y, axis = 0))\n","print(\"[Test]       Class distribution\", np.sum(test_Y, axis = 0))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hxXxHikRLsYy"},"source":["#Load Data"]},{"cell_type":"code","metadata":{"id":"JP9csrS7htgJ"},"source":["import pickle, datetime\n","import numpy as np\n","import  tensorflow as tf\n","import time\n","import os, sys\n","from sklearn import linear_model\n","import sklearn\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","from sklearn import metrics\n","\n","def get_svm_data(graphs, Y):\n","    H = graphs[:, 1, :, :]\n","    flatten_H = np.reshape(H, (H.shape[0], -1))\n","    classes = np.argmax(Y, axis = 1)\n","    return flatten_H, classes\n","\n","train_graphs, val_graphs, test_graphs, \\\n","        train_genders, val_genders, test_genders, \\\n","        train_ages, val_ages, test_ages, \\\n","        train_Y, val_Y, test_Y = pickle.load( open( pickle_path, \"rb\" ) )\n","print(\"[Training]   Graph shape, Gender shape, Ages shape, Y shape: \\n\\t\", \\\n","    train_graphs.shape, train_genders.shape, train_ages.shape, train_Y.shape)\n","print(\"[Validation] Graph shape, Gender shape, Ages shape, Y shape: \\n\\t\", \\\n","    val_graphs.shape, val_genders.shape, val_ages.shape, val_Y.shape)\n","print(\"[Test]       Graph shape, Gender shape, Ages shape, Y shape: \\n\\t\", \\\n","    test_graphs.shape, test_genders.shape, test_ages.shape, test_Y.shape)\n","\n","train_flatten_H, train_classes = get_svm_data(train_graphs, train_Y)\n","train_X = np.concatenate((train_flatten_H, train_genders, np.expand_dims(train_ages, axis=-1)), axis = 1)\n","val_flatten_H, val_classes = get_svm_data(val_graphs, val_Y)\n","val_X = np.concatenate((val_flatten_H, val_genders, np.expand_dims(val_ages, axis=-1)), axis = 1)\n","test_flatten_H, test_classes = get_svm_data(test_graphs, test_Y)\n","test_X = np.concatenate((test_flatten_H, test_genders, np.expand_dims(test_ages, axis=-1)), axis = 1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cb4FrREQL-rs"},"source":["# Train logistic regression"]},{"cell_type":"code","metadata":{"id":"OfEabGFQLygg"},"source":["print('Training....')\n","clf_l = linear_model.LogisticRegression(max_iter=1000)\n","clf_l.fit(train_X, train_classes)\n","print('Finished')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZOuVbAMTMFLf"},"source":["# Test logistic regression"]},{"cell_type":"code","metadata":{"id":"CVlNi_eVLNfj"},"source":["labels_test = test_classes\n","\n","# Evaluate SVM\n","labels_pred = clf_l.predict(test_X)\n","correct = np.sum(labels_pred==labels_test)\n","num_test = labels_pred.shape\n","three_c_acc = correct / num_test * 100\n","\n","# classify 2 as 1\n","labels_2_test, labels_2_pred = labels_test.copy(), labels_pred.copy()\n","labels_2_test[labels_2_test==2] = 1\n","labels_2_pred[labels_2_pred==2] = 1\n","\n","# calculate the accuracy\n","correct = np.sum(labels_2_test==labels_2_pred)\n","two_c_acc = correct / num_test * 100\n","print('three class acc: %.3f, two class acc: %.3f' % (three_c_acc, two_c_acc))\n","print(\"=\"*100)\n","\n","# three class confusion metrics\n","target_names = ['class 0', 'class 1', 'class 2']\n","print('three class:')\n","\n","# calculate precision, recall, f1 score\n","print(classification_report(labels_test, labels_pred, target_names=target_names, digits=4))\n","\n","# calculate AUC\n","pred_y = np.zeros((labels_pred.shape[0], 3))\n","for i, label in enumerate(labels_pred):\n","    pred_y[i, label] = 1\n","auc = sklearn.metrics.roc_auc_score(test_Y, pred_y)\n","print('AUC', auc)\n","print(\"=\"*100)\n","\n","# two class confusion metrics\n","target_names = ['class 0', 'class 1']\n","print('two class:')\n","# calculate precision, recall, f1 score\n","print(classification_report(labels_2_test, labels_2_pred, target_names=target_names, digits=4))\n","\n","# calcuate specificity\n","tn, fp, fn, tp = confusion_matrix(labels_2_test, labels_2_pred).ravel()\n","specificity = tn / (tn+fp)\n","print('specificity:', specificity)\n","\n","# calculate AUC\n","pred_y = np.zeros((labels_2_pred.shape[0], 2))\n","for i, label in enumerate(labels_2_pred):\n","    pred_y[i, label] = 1\n","auc = sklearn.metrics.roc_auc_score(labels_2_test, pred_y[:, 1])\n","print('AUC', auc)\n","print(\"=\"*100)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0o_gq800MJRL"},"source":["# Train SVM"]},{"cell_type":"code","metadata":{"id":"BGSXUhOcMOk5"},"source":["from sklearn import svm\n","\n","print('Training....')\n","clf = svm.SVC()\n","clf.fit(train_X, train_classes)\n","print('Finished')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KcSngtV_MPJL"},"source":["# Test SVM"]},{"cell_type":"code","metadata":{"id":"61zad2VqNUwY"},"source":["labels_test = test_classes\n","\n","# Evaluate SVM\n","labels_pred = clf.predict(test_X)\n","correct = np.sum(labels_pred==labels_test)\n","num_test = labels_pred.shape\n","three_c_acc = correct / num_test * 100\n","\n","# classify 2 as 1\n","labels_2_test, labels_2_pred = labels_test.copy(), labels_pred.copy()\n","labels_2_test[labels_2_test==2] = 1\n","labels_2_pred[labels_2_pred==2] = 1\n","\n","# calculate the accuracy\n","correct = np.sum(labels_2_test==labels_2_pred)\n","two_c_acc = correct / num_test * 100\n","print('three class acc: %.3f, two class acc: %.3f' % (three_c_acc, two_c_acc))\n","print(\"=\"*100)\n","\n","# three class confusion metrics\n","target_names = ['class 0', 'class 1', 'class 2']\n","print('three class:')\n","\n","# calculate precision, recall, f1 score\n","print(classification_report(labels_test, labels_pred, target_names=target_names, digits=4))\n","\n","# calculate AUC\n","pred_y = np.zeros((labels_pred.shape[0], 3))\n","for i, label in enumerate(labels_pred):\n","    pred_y[i, label] = 1\n","auc = sklearn.metrics.roc_auc_score(test_Y, pred_y)\n","print('AUC', auc)\n","print(\"=\"*100)\n","\n","# two class confusion metrics\n","target_names = ['class 0', 'class 1']\n","print('two class:')\n","# calculate precision, recall, f1 score\n","print(classification_report(labels_2_test, labels_2_pred, target_names=target_names, digits=4))\n","\n","# calcuate specificity\n","tn, fp, fn, tp = confusion_matrix(labels_2_test, labels_2_pred).ravel()\n","specificity = tn / (tn+fp)\n","print('specificity:', specificity)\n","\n","# calculate AUC\n","pred_y = np.zeros((labels_2_pred.shape[0], 2))\n","for i, label in enumerate(labels_2_pred):\n","    pred_y[i, label] = 1\n","auc = sklearn.metrics.roc_auc_score(labels_2_test, pred_y[:, 1])\n","print('AUC', auc)\n","print(\"=\"*100)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NX9e6oT6NWwC"},"source":[""],"execution_count":null,"outputs":[]}]}